{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# loading libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:11:17.611885Z","iopub.execute_input":"2022-05-20T09:11:17.612614Z","iopub.status.idle":"2022-05-20T09:11:18.979942Z","shell.execute_reply.started":"2022-05-20T09:11:17.612468Z","shell.execute_reply":"2022-05-20T09:11:18.979023Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Reading Dataset","metadata":{}},{"cell_type":"code","source":"# reading the annotated dataset\ndf = pd.read_csv('cre_root_detection.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:17:43.567268Z","iopub.execute_input":"2022-05-20T09:17:43.567760Z","iopub.status.idle":"2022-05-20T09:17:43.583569Z","shell.execute_reply.started":"2022-05-20T09:17:43.567698Z","shell.execute_reply":"2022-05-20T09:17:43.582433Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"#1- Checking Missing Values\nmissing_data = pd.DataFrame({'total_missing': df.isnull().sum(), 'perc_missing': (df.isnull().sum()/82790)*100})\nmissing_data","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:17:46.716455Z","iopub.execute_input":"2022-05-20T09:17:46.716825Z","iopub.status.idle":"2022-05-20T09:17:46.736035Z","shell.execute_reply.started":"2022-05-20T09:17:46.716785Z","shell.execute_reply":"2022-05-20T09:17:46.735146Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Handling Missing Values (freq note, freq weighted acc\t)\ndf['freq note'].fillna(df['freq note'].mode()[0], inplace = True)\ndf['freq weighted acc'].fillna(df['freq weighted acc'].mode()[0], inplace = True)\n# finding if there is any null value\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:18:15.043358Z","iopub.execute_input":"2022-05-20T09:18:15.043638Z","iopub.status.idle":"2022-05-20T09:18:15.057195Z","shell.execute_reply.started":"2022-05-20T09:18:15.043607Z","shell.execute_reply":"2022-05-20T09:18:15.056513Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Exploring 'expert assigned' variable\ndf['expert assigned'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:18:18.696831Z","iopub.execute_input":"2022-05-20T09:18:18.697292Z","iopub.status.idle":"2022-05-20T09:18:18.709336Z","shell.execute_reply.started":"2022-05-20T09:18:18.697243Z","shell.execute_reply":"2022-05-20T09:18:18.708343Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Getting index value where 'expert assigned' = 5, and then drop that value because it has less number of entries   \ndf.drop(df[(df['expert assigned'] == 5)].index , inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:21:25.364223Z","iopub.execute_input":"2022-05-20T09:21:25.364532Z","iopub.status.idle":"2022-05-20T09:21:25.372558Z","shell.execute_reply.started":"2022-05-20T09:21:25.364501Z","shell.execute_reply":"2022-05-20T09:21:25.371656Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Cohen Kappa Score computation","metadata":{}},{"cell_type":"code","source":"# we dont want to include ('title', 'certainty', 'root') in the analysis, therefore, we want to remove it\nnewDf = df.drop(['title', 'certainty', 'root'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:23:57.991950Z","iopub.execute_input":"2022-05-20T09:23:57.993076Z","iopub.status.idle":"2022-05-20T09:23:57.998342Z","shell.execute_reply.started":"2022-05-20T09:23:57.993014Z","shell.execute_reply":"2022-05-20T09:23:57.997384Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score\n# Calling DataFrame constructor  \narr = []\nfor item in newDf:\n    col = []\n    for item2 in newDf:\n        col.append(cohen_kappa_score(newDf[item], newDf[item2]))\n    arr.append(col)\n    \nmydf = pd.DataFrame(arr)\nmydf = pd.DataFrame(data=mydf.values, columns=newDf.columns)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:24:06.002835Z","iopub.execute_input":"2022-05-20T09:24:06.003145Z","iopub.status.idle":"2022-05-20T09:24:06.307755Z","shell.execute_reply.started":"2022-05-20T09:24:06.003115Z","shell.execute_reply":"2022-05-20T09:24:06.307098Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Plotting Cohen Kappa Correlation","metadata":{}},{"cell_type":"code","source":"import seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\nplt.clf()\nax = fig.add_subplot(111)\nax.set_aspect(1)\nres = sns.heatmap(mydf.values, annot=True, fmt='.2f', cmap=\"YlGnBu\", vmin=0.0, vmax=1.0)\nplt.title('Cohen Kappa Score',fontsize=12)\n\nplt.xticks([i+0.5 for i in range(mydf.values.shape[0])], [str(element) for element in mydf])\nplt.xticks(rotation=90)\n\nplt.yticks([i+0.5 for i in range(mydf.values.shape[1])], [str(element) for element in mydf])\nplt.yticks(rotation=0)\n\nplt.savefig(\"Cohen Kappa Correlation.pdf\", bbox_inches='tight', dpi=100)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:24:09.751255Z","iopub.execute_input":"2022-05-20T09:24:09.751974Z","iopub.status.idle":"2022-05-20T09:24:11.707314Z","shell.execute_reply.started":"2022-05-20T09:24:09.751905Z","shell.execute_reply":"2022-05-20T09:24:11.706691Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# GLOBAL SETTINGS","metadata":{}},{"cell_type":"code","source":"#FEATURE VECTOR SETTINGS => This will allow us to add/drop (1/0) certain features for classification \nKrumhansl_Shmuckler = 1 \nsimple_weights = 1\nAarden_Essen = 1 \nBellman_Budge = 1\nTemperly_Kostka_Payne = 1  \nas_transcribed = 1 \nfinal_note = 1 \nfreq_note = 1 \nfreq_weighted_acc = 1\ncertainty = 0 # Removing this due to low correlation\nroot = 0 # this was removed upon Danny's suggetion [Dont know what it is??]","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:02:04.702990Z","iopub.execute_input":"2022-05-20T10:02:04.703540Z","iopub.status.idle":"2022-05-20T10:02:04.709233Z","shell.execute_reply.started":"2022-05-20T10:02:04.703505Z","shell.execute_reply":"2022-05-20T10:02:04.708296Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# Defining X (features) and Y (target Class)","metadata":{}},{"cell_type":"code","source":"#X = df.drop(['expert assigned'], axis=1)  \nX = df.drop(['expert assigned', 'title'], axis=1)\n\nif(Krumhansl_Shmuckler !=  1):\n    X = X.drop(['Krumhansl-Shmuckler'], axis=1)\nif(simple_weights !=  1):\n    X = X.drop(['simple weights'], axis=1)\nif(Aarden_Essen !=  1):\n    X = X.drop(['Aarden Essen'], axis=1)\nif(Bellman_Budge !=  1):\n    X = X.drop(['Bellman Budge'], axis=1)\nif(Temperly_Kostka_Payne !=  1):  \n    X = X.drop(['Temperly Kostka Payne'], axis=1)\nif(as_transcribed !=  1):\n    X = X.drop(['as transcribed'], axis=1)\nif(final_note !=  1):\n    X = X.drop(['final_note'], axis=1)\nif(freq_note !=  1):\n    X = X.drop(['freq note'], axis=1)\nif(freq_weighted_acc !=  1):\n    X = X.drop(['freq weighted acc'], axis=1)\nif(certainty !=  1):\n    X = X.drop(['certainty'], axis=1)    \nif(root !=  1):\n    X = X.drop(['root'], axis=1)\n    \nprint(\"List of features considered: \", X.columns)\n\ny = df['expert assigned']","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:02:08.123909Z","iopub.execute_input":"2022-05-20T10:02:08.124235Z","iopub.status.idle":"2022-05-20T10:02:08.139957Z","shell.execute_reply.started":"2022-05-20T10:02:08.124204Z","shell.execute_reply":"2022-05-20T10:02:08.139219Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Taking 10% of the data out from the original to evluate the performance of the developed model","metadata":{}},{"cell_type":"code","source":"# split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nModelDataset_X, EvalationDataset_X, ModelDataset_y, EvalationDataset_y = train_test_split(X, y, test_size = 0.1, random_state = 30)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:02:11.111040Z","iopub.execute_input":"2022-05-20T10:02:11.111795Z","iopub.status.idle":"2022-05-20T10:02:11.118520Z","shell.execute_reply.started":"2022-05-20T10:02:11.111754Z","shell.execute_reply":"2022-05-20T10:02:11.117888Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"print(EvalationDataset_y.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:02:14.095905Z","iopub.execute_input":"2022-05-20T10:02:14.096372Z","iopub.status.idle":"2022-05-20T10:02:14.102756Z","shell.execute_reply.started":"2022-05-20T10:02:14.096323Z","shell.execute_reply":"2022-05-20T10:02:14.101863Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Datasets for experimentations","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Preparing dataset for Model Creation (It is kind of training dataet)\n# first random sampling\noversample = RandomOverSampler(sampling_strategy='minority')\nModelDataset_X_oversample, ModelDataset_y_oversample = oversample.fit_resample(ModelDataset_X, ModelDataset_y)   \n#print( ModelDataset_y_oversample.value_counts())\n\nsmt = SMOTE()\nModelDataset_X_smote, ModelDataset_y_smote = smt.fit_resample(ModelDataset_X_oversample, ModelDataset_y_oversample)\n\n\n# Preparing dataset for Model Evaluation (It is kind of test dataet)\n# first random sampling\noversample = RandomOverSampler(sampling_strategy='minority')\nEvalationDataset_X_oversample, EvalationDataset_y_oversample = oversample.fit_resample(EvalationDataset_X, EvalationDataset_y)\n    \n# Counting Unique values of each class after random sampling\n#smt = SMOTE()\nEvalationDataset_X_smote, EvalationDataset_y_smote = smt.fit_resample(EvalationDataset_X_oversample, EvalationDataset_y_oversample)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:02:16.449816Z","iopub.execute_input":"2022-05-20T10:02:16.450105Z","iopub.status.idle":"2022-05-20T10:02:16.487785Z","shell.execute_reply.started":"2022-05-20T10:02:16.450076Z","shell.execute_reply":"2022-05-20T10:02:16.486967Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# At this point we have the following datasets\n\n(ModelDataset_X, ModelDataset_y)\n(EvalationDataset_X, EvalationDataset_y)\n\n(ModelDataset_X_smote, ModelDataset_y_smote)\n(EvalationDataset_X_smote, EvalationDataset_y_smote)","metadata":{}},{"cell_type":"code","source":"\n# count unique values of each class\n#print(\"count unique values of each class - ModelDataset_y\")\n#print(ModelDataset_y.value_counts())\nModelDataset_y_s = ModelDataset_y.value_counts()\n\n#print(\"count unique values of each class - ModelDataset_y_smote\") \n#print(ModelDataset_y_smote.value_counts())\nModelDataset_y_smote_s = ModelDataset_y_smote.value_counts()\n\n#print(\"count unique values of each class - EvalationDataset_y\") \n#print(EvalationDataset_y.value_counts())\nEvalationDataset_y_s = EvalationDataset_y.value_counts()\n\n#print(\"count unique values of each class - EvalationDataset_y_smote\") \n#print(EvalationDataset_y_smote.value_counts())\nEvalationDataset_y_smote_s = EvalationDataset_y_smote.value_counts()\n\nmydf1 = pd.DataFrame({'note':ModelDataset_y_s.index, 'count':ModelDataset_y_s.values})\nmydf2 = pd.DataFrame({'note':ModelDataset_y_smote_s.index, 'count':ModelDataset_y_smote_s.values})\nmydf3 = pd.DataFrame({'note':EvalationDataset_y_s.index, 'count':EvalationDataset_y_s.values})\nmydf4 = pd.DataFrame({'note':EvalationDataset_y_smote_s.index, 'count':EvalationDataset_y_smote_s.values})\n\nmydf1 = mydf1.convert_dtypes(int)\nmydf2 = mydf2.convert_dtypes(int)\nmydf3 = mydf3.convert_dtypes(int)\nmydf4 = mydf4.convert_dtypes(int)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:02:20.558149Z","iopub.execute_input":"2022-05-20T10:02:20.558411Z","iopub.status.idle":"2022-05-20T10:02:20.576659Z","shell.execute_reply.started":"2022-05-20T10:02:20.558385Z","shell.execute_reply":"2022-05-20T10:02:20.575036Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Distribution of model and evaluation datasets","metadata":{}},{"cell_type":"code","source":"mdf = pd.concat([mydf1, mydf2, mydf3, mydf4], axis=1,  keys=('ModelDataset_y','ModelDataset_y_smote', \"EvalationDataset_y\", \"EvalationDataset_y_smote\"))\nmdf","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:02:23.660793Z","iopub.execute_input":"2022-05-20T10:02:23.661118Z","iopub.status.idle":"2022-05-20T10:02:23.685375Z","shell.execute_reply.started":"2022-05-20T10:02:23.661087Z","shell.execute_reply":"2022-05-20T10:02:23.684409Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"# Classification report of state-of-the-art models for root note detection ","metadata":{}},{"cell_type":"code","source":"\nprint(\"Classification Report - Krumhansl-Shmuckler\")\nprint(classification_report(EvalationDataset_y, EvalationDataset_X[\"Krumhansl-Shmuckler\"], labels=[0,2,4,7,9]))\n\nprint(\"Classification Report - simple weights\")\nprint(classification_report(EvalationDataset_y, EvalationDataset_X[\"simple weights\"], labels=[0,2,4,7,9]))\n\nprint(\"Classification Report - Aarden Essen\")\nprint(classification_report(EvalationDataset_y, EvalationDataset_X[\"Aarden Essen\"], labels=[0,2,4,7,9]))\n\nprint(\"Classification Report - Bellman Budge\")\nprint(classification_report(EvalationDataset_y, EvalationDataset_X[\"Bellman Budge\"], labels=[0,2,4,7,9]))\n\nprint(\"Classification Report - Temperly Kostka Payne\")\nprint(classification_report(EvalationDataset_y, EvalationDataset_X[\"Temperly Kostka Payne\"], labels=[0,2,4,7,9]))\n\n\nprint(\"Classification Report - as transcribed\")\nprint(classification_report(EvalationDataset_y, EvalationDataset_X['as transcribed'], labels=[0,2,4,7,9]))\n\n\nprint(\"Classification Report - 'final_note'\")\nprint(classification_report(EvalationDataset_y, EvalationDataset_X['final_note'], labels=[0,2,4,7,9]))\n\n\nprint(\"Classification Report - 'freq note'\")\nprint(classification_report(EvalationDataset_y, EvalationDataset_X['freq note'], labels=[0,2,4,7,9]))\n\nprint(\"Classification Report - 'freq weighted acc'\")\nprint(classification_report(EvalationDataset_y, EvalationDataset_X['freq weighted acc'], labels=[0,2,4,7,9]))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:02:26.904760Z","iopub.execute_input":"2022-05-20T10:02:26.905109Z","iopub.status.idle":"2022-05-20T10:02:26.968306Z","shell.execute_reply.started":"2022-05-20T10:02:26.905071Z","shell.execute_reply":"2022-05-20T10:02:26.967282Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# Factorial Design Experimental Setup\n**Grid-based hyperparameter tuning for developing the optimized model**","metadata":{}},{"cell_type":"code","source":"# import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n# import Random Forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\n# train a Gaussian Naive Bayes classifier on the training set\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\n\nsvm_param_grid = {\n             'C': [0.1, 0.5, 1.0],\n             'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n             'degree': [2, 3, 5],\n             'gamma': ['auto', 'scale'],\n             'tol': [1e-5, 1e-3, 1e-2],\n             'max_iter': [-1, 5, 10]\n}\n\nRandomForest_param_grid = {\n    'max_depth': [2, 3, 4, 5, 9, 10, 11,12, 15, 20, 22, 23, 30, 60],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 25],\n    'criterion': ['gini', 'entropy'],\n}\n\n\nDecisionTree_param_grid = {\n    'max_depth': [2, 3, 4, 5, 10, 15, 30, 60],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'criterion': ['gini', 'entropy'],\n}\n\nNB_param_grid = {\n                'var_smoothing': np.logspace(0,-9, num=100)\n                }\n\n\nmodel_param = {\n  'DecisionTree':{\n      'model': DecisionTreeClassifier(),\n      'params': DecisionTree_param_grid,\n  },\n    \n  'RandomForest': {\n          'model': RandomForestClassifier(),\n          'params': RandomForest_param_grid,\n    },\n    \n  'NB': {\n          'model': GaussianNB(),\n          'params': NB_param_grid,\n    },\n   \n  #'SVM': {\n  #         'model':   svm.SVC(),\n  #         'params': svm_param_grid,    \n  # }    \n}","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:07:25.414453Z","iopub.execute_input":"2022-05-20T10:07:25.414760Z","iopub.status.idle":"2022-05-20T10:07:25.432719Z","shell.execute_reply.started":"2022-05-20T10:07:25.414730Z","shell.execute_reply":"2022-05-20T10:07:25.431970Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# function to be called for evaluating each model defined above\ndef getBestModel (X, y):\n    results = []\n    for model, param in model_param.items():\n        clf = GridSearchCV(param['model'], param['params'], cv= 10, scoring='f1_macro')\n        clf.fit(X, y.ravel())\n        results.append(\n                            {\n                            'model': param['model'],\n                            'best_score': clf.best_score_,\n                            'best_params': clf.best_params_,\n                            }\n                      )\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:28:07.165022Z","iopub.execute_input":"2022-03-06T17:28:07.165962Z","iopub.status.idle":"2022-03-06T17:28:07.172388Z","shell.execute_reply.started":"2022-03-06T17:28:07.165894Z","shell.execute_reply":"2022-03-06T17:28:07.171695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The following experimentation should be exectued twice \n\n1. **with `as transcribe` feature**\n1. **without `as transcribe` feature** ","metadata":{}},{"cell_type":"markdown","source":"# Factorial Design Experiment - GridSearch - Implementation\n# finding best models on orgininal dataset (ModelDataset_X, ModelDataset_y)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n#ModelDataset_X, ModelDataset_y,\n#ModelDataset_X_smote, ModelDataset_y_smote,\n#EvalationDataset_X, EvalationDataset_y,\n#EvalationDataset_X_smote, EvalationDataset_y_smote\nresults = getBestModel(ModelDataset_X, ModelDataset_y)\n\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T18:07:42.334473Z","iopub.execute_input":"2022-03-06T18:07:42.334801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# loading best models of DecisionTree, RandomForest, and GaussianNB","metadata":{}},{"cell_type":"code","source":"result1 = pd.DataFrame(results)\nresult1.sort_values(by='best_score',ascending=False, inplace=True)\nresult1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the static indices are used based on the above results\nbestModel_DecisionTreeClassifier = results[0]['model']\nbestModel_RandomForestClassifier = results[1]['model']\nbestModel_GaussianNB = results[2]['model']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fitting the best models\n**priting its classification report for further analysis**","metadata":{}},{"cell_type":"code","source":"bestModel_DecisionTreeClassifier.fit(ModelDataset_X, ModelDataset_y.ravel())\nbestModel_RandomForestClassifier.fit(ModelDataset_X, ModelDataset_y.ravel())\nbestModel_GaussianNB.fit(ModelDataset_X, ModelDataset_y.ravel())\n\nprint(bestModel_RandomForestClassifier)\nprint(bestModel_DecisionTreeClassifier)\nprint(bestModel_GaussianNB)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Best models evaluation on unseen dataset (EvalationDataset_X and EvalationDataset_X_smote)","metadata":{}},{"cell_type":"code","source":"# apply the model\nEvalationDataset_y_pred = bestModel_RandomForestClassifier.predict(EvalationDataset_X)\nprint(classification_report(EvalationDataset_y, EvalationDataset_y_pred, labels=[0,2,4,7,9]))\nEvalationDataset_y_pred = bestModel_RandomForestClassifier.predict(EvalationDataset_X_smote)\nprint(classification_report(EvalationDataset_y_smote, EvalationDataset_y_pred, labels=[0,2,4,7,9]))\n\n\nEvalationDataset_y_pred = bestModel_DecisionTreeClassifier.predict(EvalationDataset_X)\nprint(classification_report(EvalationDataset_y, EvalationDataset_y_pred, labels=[0,2,4,7,9]))\nEvalationDataset_y_pred = bestModel_DecisionTreeClassifier.predict(EvalationDataset_X_smote)\nprint(classification_report(EvalationDataset_y_smote, EvalationDataset_y_pred, labels=[0,2,4,7,9]))\n\nEvalationDataset_y_pred = bestModel_GaussianNB.predict(EvalationDataset_X)\nprint(classification_report(EvalationDataset_y, EvalationDataset_y_pred, labels=[0,2,4,7,9]))\nEvalationDataset_y_pred = bestModel_GaussianNB.predict(EvalationDataset_X_smote)\nprint(classification_report(EvalationDataset_y_smote, EvalationDataset_y_pred, labels=[0,2,4,7,9]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bestModel = results[0]['model']\nprint(bestModel)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EvalationDataset_y\nprint(\"count unique values of each class - EvalationDataset_y\") \nprint(EvalationDataset_y.value_counts())\n\n\nprint(\"count unique values of each class - EvalationDataset_y_smote\") \nprint(EvalationDataset_y_smote.value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:25:15.843867Z","iopub.execute_input":"2022-03-06T17:25:15.844162Z","iopub.status.idle":"2022-03-06T17:25:15.853235Z","shell.execute_reply.started":"2022-03-06T17:25:15.844134Z","shell.execute_reply":"2022-03-06T17:25:15.852137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Factorial Design Experiment - GridSearch - Implementation\n**finding best models on orgininal dataset (ModelDataset_X_smote, ModelDataset_y_smote)**","metadata":{}},{"cell_type":"code","source":"my_results = getBestModel(ModelDataset_X_smote, ModelDataset_y_smote)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:25:24.661499Z","iopub.execute_input":"2022-03-06T17:25:24.661797Z","iopub.status.idle":"2022-03-06T17:25:24.681842Z","shell.execute_reply.started":"2022-03-06T17:25:24.661768Z","shell.execute_reply":"2022-03-06T17:25:24.681062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_results1 = pd.DataFrame(my_results)\nmy_results1.sort_values(by='best_score',ascending=False, inplace=True)\nmy_results1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Best models based on previous results","metadata":{}},{"cell_type":"code","source":"bestModel_s_DecisionTreeClassifier = my_results[0]['model']\nbestModel_s_RandomForestClassifier = my_results[1]['model']\nbestModel_s_GaussianNB = my_results[2]['model']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bestModel_s_DecisionTreeClassifier.fit(ModelDataset_X_smote, ModelDataset_y_smote.ravel())\nbestModel_s_RandomForestClassifier.fit(ModelDataset_X_smote, ModelDataset_y_smote.ravel())\nbestModel_s_GaussianNB.fit(ModelDataset_X_smote, ModelDataset_y_smote.ravel())\n\nprint(bestModel_s_DecisionTreeClassifier)\nprint(bestModel_s_RandomForestClassifier)\nprint(bestModel_s_GaussianNB)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bestModel2 = my_results[0]['model']\nbestModel2.fit(ModelDataset_X_smote, ModelDataset_y_smote.ravel())\nmy_results = pd.DataFrame(my_results)\nmy_results.sort_values(by='best_score',ascending=False, inplace=True)\nprint(\"Results with smote \")\nmy_results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Best models evaluation on unseen dataset (EvalationDataset_X and EvalationDataset_X_smote)","metadata":{}},{"cell_type":"code","source":"print(\"Randomforest - Classifier\")\n\nEvalationDataset_y_pred = bestModel_s_RandomForestClassifier.predict(EvalationDataset_X)\nprint(classification_report(EvalationDataset_y, EvalationDataset_y_pred, labels=[0,2,4,7,9]))\nEvalationDataset_y_pred = bestModel_s_RandomForestClassifier.predict(EvalationDataset_X_smote)\nprint(classification_report(EvalationDataset_y_smote, EvalationDataset_y_pred, labels=[0,2,4,7,9]))\n\nprint(\"Decision Tree - Classifier\")\nEvalationDataset_y_pred = bestModel_s_DecisionTreeClassifier.predict(EvalationDataset_X)\nprint(classification_report(EvalationDataset_y, EvalationDataset_y_pred, labels=[0,2,4,7,9]))\nEvalationDataset_y_pred = bestModel_s_DecisionTreeClassifier.predict(EvalationDataset_X_smote)\nprint(classification_report(EvalationDataset_y_smote, EvalationDataset_y_pred, labels=[0,2,4,7,9]))\n\n\nprint(\"Naive Bayes - Classifier\")\nEvalationDataset_y_pred = bestModel_s_GaussianNB.predict(EvalationDataset_X)\nprint(classification_report(EvalationDataset_y, EvalationDataset_y_pred, labels=[0,2,4,7,9]))\nEvalationDataset_y_pred = bestModel_s_GaussianNB.predict(EvalationDataset_X_smote)\nprint(classification_report(EvalationDataset_y_smote, EvalationDataset_y_pred, labels=[0,2,4,7,9]))","metadata":{},"execution_count":null,"outputs":[]}]}
