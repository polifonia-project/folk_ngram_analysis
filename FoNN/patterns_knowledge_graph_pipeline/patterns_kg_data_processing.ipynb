{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Polifonia [Patterns Knowledge Graph](https://github.com/polifonia-project/patterns-knowledge-graph) (KG) ingest pipeline. Step 2: Data processing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is step 2 of a 2-step data preprocessing pipeline. Step 1 can be found in ```./patters_kg_data_extraction.ipynb``` notebook and must be applied to any input corpus before running this notebook.\n",
    "\n",
    "After running ```patters_kg_data_extraction.ipynb```, point 'kg_data_dir' path in the cell below to ```/[corpus name]/kg_pipeline_input_data``` dir (which contains files outputted by the first preprocessing step).\n",
    "\n",
    "NOTE: This notebook must be run in full for each individual pattern length. Pattern length is set via 'n' variable in cell below. E.g.: In our sample data below, ```../FoNN/mtc_ann_corpus/kg_pipeline_input_data``` contains patterns of 4, 5, and 6 elements in length outputted by ```patters_kg_data_extraction.ipynb```. Accordingly, users must run this notebook in three passes, for n=4, n=5 and n=6.\n",
    "\n",
    "Output of this notebook is written to ```../FoNN/mtc_ann_corpus/kg_pipeline_output_data```.\n",
    "To create a KG from this output data, please see Polifonia [Patterns Knowledge Graph](https://github.com/polifonia-project/patterns-knowledge-graph) repo for further information.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2580 patterns extracted of length 5\n"
     ]
    }
   ],
   "source": [
    "# Load pattern occurrences matrix from kg_pipeline_input_data directory:\n",
    "\n",
    "kg_data_in_dir = '../mtc_ann_corpus/kg_pipeline_input_data'\n",
    "n = 5  # set pattern length under investigation\n",
    "in_file = f'{n}gram_patterns.pkl'\n",
    "in_path = kg_data_in_dir + '/' + in_file\n",
    "pattern_occurrences = pd.read_pickle(in_path)\n",
    "# print number of patterns in input matrices\n",
    "print(f\"{len(pattern_occurrences)} patterns extracted of length {n}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780 patterns retained after filtering.\n"
     ]
    }
   ],
   "source": [
    "# filter pattern occurrences matrix, retaining patterns which occur twice or more\n",
    "occurrence_counts = pattern_occurrences.astype('Sparse[int16, 0]')\n",
    "occurrence_counts.fillna(value=0, inplace=True)\n",
    "occurrence_counts['freq'] = occurrence_counts.sum(axis=1)\n",
    "filtered_pattern_occurrences = occurrence_counts[occurrence_counts['freq'] >= 2]\n",
    "# print number of patterns retained after filtration\n",
    "print(f\"{len(filtered_pattern_occurrences)} patterns retained after filtering.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output:\n",
      "[(1, 1, 1, 1, 1), (1, 1, 1, 1, 2), (1, 1, 1, 1, 3)]\n"
     ]
    }
   ],
   "source": [
    "# change index col type from numpy array to tuples\n",
    "patterns_reformatted = [tuple(i) for i in filtered_pattern_occurrences.index]\n",
    "sample_output = patterns_reformatted[:3]\n",
    "print(f\"Sample output:\")\n",
    "print(sample_output)\n",
    "filtered_patterns = set(patterns_reformatted)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "# read locations dict outputted by patterns_kg_data_extraction.ipynb\n",
    "locations_filename = f'{n}gram_locations.pkl'\n",
    "locations_in_path = kg_data_in_dir + '/' + locations_filename\n",
    "with open(locations_in_path, 'rb') as locations_raw:\n",
    "    locations = pickle.load(locations_raw)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "# extract tune titles and id numbers from locations data:\n",
    "\n",
    "# from string import digits\n",
    "titles  = [tune for tune in locations]  # titles  = [tune.rstrip(digits) for tune in locations]\n",
    "# extract tune id numbers:\n",
    "tune_id_nums = [''.join(i for i in tune if not  i.isalpha()) for tune in locations]\n",
    "# create lookup dict of tune id numbers (keys): tune titles (vals)\n",
    "id_dict = dict(zip(tune_id_nums, titles))\n",
    "# create second dict, of tune id numbers (keys): to pattern locations data (vals)\n",
    "id_dict_filenames = dict(zip(tune_id_nums, list(locations)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output for single tune:\n",
      "identifiers: 072355_01:\n",
      "pattern locations: {(4, 4, 4, 5, 5): [0], (4, 4, 5, 5, 5): [1], (4, 5, 5, 5, 4): [2], (5, 5, 5, 4, 3): [3], (5, 5, 4, 3, 3): [4], (5, 4, 3, 3, 1): [5], (4, 3, 3, 1, 1): [6], (3, 3, 1, 1, 1): [7], (3, 1, 1, 1, 1): [8], (1, 1, 1, 1, 3): [9], (1, 1, 1, 3, 3): [10], (1, 1, 3, 3, 2): [11], (1, 3, 3, 2, 2): [12], (3, 3, 2, 2, 2): [13, 37], (3, 2, 2, 2, 2): [14, 38], (2, 2, 2, 2, 3): [15], (2, 2, 2, 3, 3): [16], (2, 2, 3, 3, 2): [17, 35], (2, 3, 3, 2, 1): [18], (3, 3, 2, 1, 1): [19], (3, 2, 1, 1, 1): [20], (2, 1, 1, 1, 2): [21], (1, 1, 1, 2, 2): [22], (1, 1, 2, 2, 2): [23], (1, 2, 2, 2, 2): [24], (2, 2, 2, 2, 5): [25], (2, 2, 2, 5, 5): [26], (2, 2, 5, 5, 5): [27], (2, 5, 5, 5, 3): [28], (5, 5, 5, 3, 2): [29], (5, 5, 3, 2, 1): [30], (5, 3, 2, 1, 2): [31], (3, 2, 1, 2, 2): [32], (2, 1, 2, 2, 3): [33], (1, 2, 2, 3, 3): [34], (2, 3, 3, 2, 2): [36], (2, 2, 2, 2, 1): [39], (2, 2, 2, 1, 1): [40], (2, 2, 1, 1, 1): [41], (2, 1, 1, 1, 1): [42], (1, 1, 1, 1, 1): [43]}\n"
     ]
    }
   ],
   "source": [
    "# filter locations dict\n",
    "\n",
    "filtered_locations = {}\n",
    "for tune in locations:\n",
    "    # read value, which holds a dict of patterns and locations\n",
    "    pattern_locations = locations[tune]\n",
    "    # filter\n",
    "    filtered = {pattern: pattern_locations[pattern] for pattern in pattern_locations if pattern in filtered_patterns}\n",
    "    filtered_locations[tune] = filtered\n",
    "\n",
    "# check output\n",
    "locations_out = dict(zip(list(id_dict), list(filtered_locations.values())))\n",
    "print('Sample output for single tune:')\n",
    "for k, v in list(locations_out.items())[:1]:\n",
    "    print(f\"identifiers: {k}:\\npattern locations: {v}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "# calculate tune lengths (length of feature sequences):\n",
    "\n",
    "import os\n",
    "\n",
    "def calculate_tune_lengths(target_dir):\n",
    "    # calculate length of all feature sequences in input corpus\n",
    "    tune_durations = []\n",
    "    for file_name in os.listdir(target_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = f\"{target_dir}/{file_name}\"\n",
    "            with open(file_path) as content:\n",
    "                counter = len(content.readlines()) + 1\n",
    "                tune_durations.append(counter)\n",
    "    # store in dict per tune id numbers (keys): tune durations (vals)\n",
    "    tune_durations = dict(zip(list(id_dict), tune_durations))\n",
    "\n",
    "    return tune_durations\n",
    "\n",
    "# run:\n",
    "feat_seq_path = '../mtc_ann_corpus/feature_sequence_data/duration_weighted'\n",
    "tune_lengths = calculate_tune_lengths(feat_seq_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "identifiers: 072355_01: \n",
      " data (diatonic scale degree feature sequence values): [4, 4, 4, 5, 5, 5, 4, 3, 3, 1, 1, 1, 1, 3, 3, 2, 2, 2, 2, 3, 3, 2, 1, 1, 1, 2, 2, 2, 2, 5, 5, 5, 3, 2, 1, 2, 2, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# Extract diatonic scale degree sequences from csv feature sequence files for all tunes\n",
    "# NOTE: If investigating a different musical feature, please specify below via 'feature' variable.\n",
    "# A full list of FoNN features is available in ../README.md\n",
    "feature = 'diatonic_scale_degree'\n",
    "filenames = [filename[:-4] for filename in os.listdir(feat_seq_path) if filename.endswith('.csv')]\n",
    "inpaths = [f\"{feat_seq_path}/{filename}\" for filename in os.listdir(feat_seq_path) if filename.endswith(\".csv\")]\n",
    "# identify csv col names\n",
    "with open(inpaths[0]) as testfile:\n",
    "    csv_reader = csv.reader(testfile, delimiter=',')\n",
    "    cols = next(csv_reader)\n",
    "    colsmap = {col_name: i for i, col_name in enumerate(cols)}\n",
    "    assert feature in colsmap\n",
    "# identify index of target column\n",
    "target_col_idx = colsmap[feature]\n",
    "# extract target column from all tunes to list\n",
    "numeric_data = [\n",
    "    np.genfromtxt(\n",
    "        path,\n",
    "        dtype=str,\n",
    "        delimiter=',',\n",
    "        usecols=target_col_idx,\n",
    "            skip_header=1)\n",
    "        for path in inpaths\n",
    "    ]\n",
    "# convert feature sequence data from numeric to string\n",
    "formatted_feat_seq_data = []\n",
    "for i in numeric_data:\n",
    "    numeric = []\n",
    "    for j in i:\n",
    "        num = int(float(j)) if j != '' else 0\n",
    "        numeric.append(num)\n",
    "    formatted_feat_seq_data.append(numeric)\n",
    "# store in dict per tune id numbers (keys): feature sequence data (vals)\n",
    "feat_seq_data =dict(zip(tune_id_nums, formatted_feat_seq_data))\n",
    "\n",
    "# confirm order of dict items matches 'locations' variable above\n",
    "print(filenames == list(locations))\n",
    "# check output\n",
    "for k, v in list(feat_seq_data.items())[:1]:\n",
    "    print(f\"identifiers: {k}: \\n data ({' '.join(feature.split('_'))} feature sequence values): {v}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test output mappings for single tune:\n",
      "identifiers: 072355_01\n",
      "title: NLB072355_01\n",
      "data: [4, 4, 4, 5, 5, 5, 4, 3, 3, 1, 1, 1, 1, 3, 3, 2, 2, 2, 2, 3, 3, 2, 1, 1, 1, 2, 2, 2, 2, 5, 5, 5, 3, 2, 1, 2, 2, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 1]\n",
      "duration: 50\n",
      "pattern_locations: {(4, 4, 4, 5, 5): [0], (4, 4, 5, 5, 5): [1], (4, 5, 5, 5, 4): [2], (5, 5, 5, 4, 3): [3], (5, 5, 4, 3, 3): [4], (5, 4, 3, 3, 1): [5], (4, 3, 3, 1, 1): [6], (3, 3, 1, 1, 1): [7], (3, 1, 1, 1, 1): [8], (1, 1, 1, 1, 3): [9], (1, 1, 1, 3, 3): [10], (1, 1, 3, 3, 2): [11], (1, 3, 3, 2, 2): [12], (3, 3, 2, 2, 2): [13, 37], (3, 2, 2, 2, 2): [14, 38], (2, 2, 2, 2, 3): [15], (2, 2, 2, 3, 3): [16], (2, 2, 3, 3, 2): [17, 35], (2, 3, 3, 2, 1): [18], (3, 3, 2, 1, 1): [19], (3, 2, 1, 1, 1): [20], (2, 1, 1, 1, 2): [21], (1, 1, 1, 2, 2): [22], (1, 1, 2, 2, 2): [23], (1, 2, 2, 2, 2): [24], (2, 2, 2, 2, 5): [25], (2, 2, 2, 5, 5): [26], (2, 2, 5, 5, 5): [27], (2, 5, 5, 5, 3): [28], (5, 5, 5, 3, 2): [29], (5, 5, 3, 2, 1): [30], (5, 3, 2, 1, 2): [31], (3, 2, 1, 2, 2): [32], (2, 1, 2, 2, 3): [33], (1, 2, 2, 3, 3): [34], (2, 3, 3, 2, 2): [36], (2, 2, 2, 2, 1): [39], (2, 2, 2, 1, 1): [40], (2, 2, 1, 1, 1): [41], (2, 1, 1, 1, 1): [42], (1, 1, 1, 1, 1): [43]}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup namedtuple to store output for KG setup:\n",
    "from typing import NamedTuple\n",
    "KG_Data = NamedTuple('KG_Data', [\n",
    "    ('identifiers', str),           # tune id number\n",
    "    ('title', str),                 # tune title\n",
    "    ('feature', str),               # name of musical feature under investigation\n",
    "    ('level', str),                 # level of granularity of input data. Must be manually defined below in KG_Data instance.\n",
    "    ('n_vals', tuple),              # range of n-values (i.e.: pattern lengths) under investigation. Must be manually defined below in KG_Data instance.\n",
    "    ('duration', int),              # length of tune feature sequence\n",
    "    ('pattern_locations', dict),    # dict of patterns and their locations within the feature sequence\n",
    "    ('data', list)                  # feature sequence data content\n",
    "])\n",
    "\n",
    "# to illustrate and test output mappings,\n",
    "# use tune id numbers (common keys) to look up values of each of the above data fields from dicts created above\n",
    "print(\"Test output mappings for single tune:\")\n",
    "for tune_id_num in list(id_dict)[:1]:\n",
    "    print(f\"identifiers: {tune_id_num}\")\n",
    "    print(f\"title: {id_dict[tune_id_num]}\")\n",
    "    print(f\"data: {feat_seq_data[tune_id_num]}\")\n",
    "    print(f\"duration: {tune_lengths[tune_id_num]}\")\n",
    "    print(f\"pattern_locations: {locations_out[tune_id_num]}\")\n",
    "    print('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test output:\n",
      "KG_Data(identifiers='072355_01', title='NLB072355_01', feature='diatonic_scale_degree', level='duration-weighted note-level', n_vals=(4, 5, 6), duration=50, pattern_locations={(4, 4, 4, 5, 5): [0], (4, 4, 5, 5, 5): [1], (4, 5, 5, 5, 4): [2], (5, 5, 5, 4, 3): [3], (5, 5, 4, 3, 3): [4], (5, 4, 3, 3, 1): [5], (4, 3, 3, 1, 1): [6], (3, 3, 1, 1, 1): [7], (3, 1, 1, 1, 1): [8], (1, 1, 1, 1, 3): [9], (1, 1, 1, 3, 3): [10], (1, 1, 3, 3, 2): [11], (1, 3, 3, 2, 2): [12], (3, 3, 2, 2, 2): [13, 37], (3, 2, 2, 2, 2): [14, 38], (2, 2, 2, 2, 3): [15], (2, 2, 2, 3, 3): [16], (2, 2, 3, 3, 2): [17, 35], (2, 3, 3, 2, 1): [18], (3, 3, 2, 1, 1): [19], (3, 2, 1, 1, 1): [20], (2, 1, 1, 1, 2): [21], (1, 1, 1, 2, 2): [22], (1, 1, 2, 2, 2): [23], (1, 2, 2, 2, 2): [24], (2, 2, 2, 2, 5): [25], (2, 2, 2, 5, 5): [26], (2, 2, 5, 5, 5): [27], (2, 5, 5, 5, 3): [28], (5, 5, 5, 3, 2): [29], (5, 5, 3, 2, 1): [30], (5, 3, 2, 1, 2): [31], (3, 2, 1, 2, 2): [32], (2, 1, 2, 2, 3): [33], (1, 2, 2, 3, 3): [34], (2, 3, 3, 2, 2): [36], (2, 2, 2, 2, 1): [39], (2, 2, 2, 1, 1): [40], (2, 2, 1, 1, 1): [41], (2, 1, 1, 1, 1): [42], (1, 1, 1, 1, 1): [43]}, data=[4, 4, 4, 5, 5, 5, 4, 3, 3, 1, 1, 1, 1, 3, 3, 2, 2, 2, 2, 3, 3, 2, 1, 1, 1, 2, 2, 2, 2, 5, 5, 5, 3, 2, 1, 2, 2, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 1])\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "# populate output class instances\n",
    "output = []\n",
    "for tune_id_num in id_dict:\n",
    "    tune_data = KG_Data(\n",
    "        identifiers= tune_id_num,\n",
    "        title=id_dict[tune_id_num],\n",
    "        feature=feature,\n",
    "        level='duration-weighted note-level',\n",
    "        data=feat_seq_data[tune_id_num],\n",
    "        n_vals=(4, 5, 6),\n",
    "        duration=tune_lengths[tune_id_num],\n",
    "        pattern_locations=locations_out[tune_id_num])\n",
    "    output.append(tune_data)\n",
    "\n",
    "print(\"Test output:\")\n",
    "print(output[0])\n",
    "print(len(output))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "# write output\n",
    "output_filename = f'{n}gram_kg_data.pkl'\n",
    "out_dir = '../mtc_ann_corpus/kg_pipeline_output_data'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "out_path = out_dir + '/' + output_filename\n",
    "with open(out_path, 'wb') as f_out:\n",
    "    pickle.dump(output, f_out)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}